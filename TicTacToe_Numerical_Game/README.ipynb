{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TicTacToe Numerical Reinforced Learning Game Agent\n",
    "\n",
    "#### Background\n",
    "Conventional TicTacToe game involes just 2 kinds of inputs (X or O). However, in an numerical TicTacToe game, we play with numbers.\n",
    "- The grid looks like a 3X3 matrix.\n",
    "- One player gets odd numbers (1,3,5,7,9) to play\n",
    "- Another player gets even numbers (2,4,6,8) to play\n",
    "- Ever player moves in alternate fashion (one-after-the-other)\n",
    "- Who ever is able to complete a sum of 15 in any one row/column/major diagonal, WINS the game.\n",
    "- **IMPORTANT : The odd player always plays first.**\n",
    "\n",
    "Example:\n",
    "\n",
    "<img src=\"images/img1.JPG\">\n",
    "\n",
    "\n",
    "#### Environment\n",
    "- We have presented 2 agents here, one who plays EVEN numbers and other who plays ODD numbers\n",
    "- The environments in these cases are complementary to the agent.\n",
    "- `Agent-Learning.ipynb` file in each of the directories, has the environment code embedded as `class TicTacToe`\n",
    "- For playing with the learnt agent, a different Environment file is provided as `TCGame_Env_Game.py` which helps play a `user_input` driven game.\n",
    "\n",
    "#### Reward Structure\n",
    "- For **every move** the agent gets a `-1 reward`\n",
    "- If because of a move, the **Agent wins** (GAME ENDS) he gets a `+10 reward`\n",
    "- If because of a move, the **Agent lost** (GAME ENDS) it gets a `-10 reward`\n",
    "- If the (GAME ENDS) as a **Draw**, the agent gets a `0 reward`\n",
    "\n",
    "\n",
    "#### RL Agent\n",
    "- RL Agent Learning code has been provided as a separate python notebook\n",
    "- RL Agent utilizes the concept of Q-Learning, in learning the best action to take based on the current state of the play board.\n",
    "- Since an agent will take an action based on the moves played after the environment has played, therefore, there are considerably finite `state` that will ever observe.\n",
    "- Agent maintains an in-memory data structure\n",
    " - where in key is the state seen by the agent, \n",
    " - as sub-key, there are possibles moves that the agent can do in such cases. \n",
    " - Each move is thereafter associated with a value, which we call as the Q-Value \n",
    "\n",
    "- (Also known as the action value : Synonymous to `q(s,a)` in RL equation. In short`q(s,a)` is the weighted sum of immediate reward and expected future rewards that an agent will gain by taking `action (a)` when in `state (s)`\n",
    "\n",
    "\n",
    "#### Algorithm (Q Learning - Deterministic States - (without Neural Networks)\n",
    "\n",
    "Following are the 2 main equations in Reinforcement Learning environment\n",
    "\n",
    "State Value Function : $V(s) = \\pi(a,s)Q_\\pi(s,a)$\n",
    "\n",
    "Action Value Function (q-value) : $Q_\\pi(s,a) = P(\\hat{s},r | s,a)[ r + \\gamma( V(\\hat{s}) ) ]$\n",
    "\n",
    "Combined Equation (based on state value) : $V(s) = \\pi(a,s)P(\\hat{s},r | s,a)[ r + \\gamma( V(\\hat{s}) ) ]$\n",
    "\n",
    "The concept of Q-Learning comes under **Model Free** Reinforcement Learning. With Model Free what is meant is that the model of the environment is not known, precisely \n",
    "\n",
    "$P(\\hat{s},r | s,a)$ - Probability of moving to state ($\\hat{s}$) and getting a reward ($r$) given present ($s$) and ($a$).\n",
    "\n",
    "The above probabilistic map of environment, is not available in **Model Free** environments.\n",
    "\n",
    "(To be updated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
