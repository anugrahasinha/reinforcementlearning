{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T06:45:01.958820Z",
     "start_time": "2019-02-26T06:45:01.190677Z"
    }
   },
   "outputs": [],
   "source": [
    "from TCGame_Env_GAME import TicTacToe\n",
    "import collections\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import collections\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T06:45:01.968076Z",
     "start_time": "2019-02-26T06:45:01.958820Z"
    }
   },
   "outputs": [],
   "source": [
    "env = TicTacToe()\n",
    "current_game_state = env.state.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T06:45:06.827476Z",
     "start_time": "2019-02-26T06:45:02.068070Z"
    }
   },
   "outputs": [],
   "source": [
    "# load model - Q_dict #\n",
    "with open(\"../Agent To Agent Competition/even_Agent_20190226_145324/even_Q_Dict_Model_20190226_150439.pickle\",\"rb\") as f:\n",
    "    Q_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T06:45:06.839073Z",
     "start_time": "2019-02-26T06:45:06.827476Z"
    }
   },
   "outputs": [],
   "source": [
    "def Q_state(state):\n",
    "    return ('-'.join(str(e) for e in state)).replace('nan','x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T06:45:07.124621Z",
     "start_time": "2019-02-26T06:45:06.843074Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_agent_action(game_state,q_dict_ref,random_select=False):\n",
    "    if random_select:\n",
    "        proposed_action = random.choice([ac for itr,ac in enumerate(env.action_space(game_state)[0])])\n",
    "    else:\n",
    "        proposed_action = max(q_dict_ref[Q_state(game_state)],key=q_dict_ref[Q_state(game_state)].get)\n",
    "    return proposed_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T06:45:07.243054Z",
     "start_time": "2019-02-26T06:45:07.130627Z"
    }
   },
   "outputs": [],
   "source": [
    "def execute_action(game_state,action,from_user=True):\n",
    "    # action = (position,number)\n",
    "    agent_allowed, user_allowed = env.action_space(game_state)\n",
    "    if from_user and action in user_allowed:\n",
    "        return env.state_transition(game_state,action)\n",
    "    elif not from_user and action in agent_allowed:\n",
    "        return env.state_transition(game_state,action)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid input : %s , from_user : %s\" %(str(action),str(from_user)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T06:45:07.404125Z",
     "start_time": "2019-02-26T06:45:07.247037Z"
    }
   },
   "outputs": [],
   "source": [
    "def valid_actions(state):\n",
    "\n",
    "    valid_Actions = []\n",
    "    \n",
    "    valid_Actions = [i for i in env.action_space(state)[0]]\n",
    "    return valid_Actions\n",
    "\n",
    "def add_to_dict(state,q_dict_ref):\n",
    "    state1 = Q_state(state)\n",
    "    \n",
    "    valid_act = valid_actions(state)\n",
    "    if state1 not in q_dict_ref.keys():\n",
    "        print(\"State = %s was seen by agent for the first time :-0, adding it for learning\" %(state1))\n",
    "        for action in valid_act:\n",
    "            q_dict_ref[state1][action]=0\n",
    "    \n",
    "    return q_dict_ref"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Testing) ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T06:45:14.867080Z",
     "start_time": "2019-02-26T06:45:14.859079Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan, nan, nan, nan, nan, nan, nan, nan, nan]\n"
     ]
    }
   ],
   "source": [
    "print(env.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just Play the game #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T06:48:04.618952Z",
     "start_time": "2019-02-26T06:46:42.597941Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ['X' 'X' 'X']\n",
      "    ['X' 'X' 'X']\n",
      "    ['X' 'X' 'X']\n",
      "Give user action (position,number):(0,7)\n",
      "After user input : Position = 0, number = 7\n",
      "    ['7' 'X' 'X']\n",
      "    ['X' 'X' 'X']\n",
      "    ['X' 'X' 'X']\n",
      "Agent Action : Position = 8, number = 6\n",
      "After agent action, state\n",
      "    ['7' 'X' 'X']\n",
      "    ['X' 'X' 'X']\n",
      "    ['X' 'X' '6']\n",
      "Give user action (position,number):(4,1)\n",
      "After user input : Position = 4, number = 1\n",
      "    ['7' 'X' 'X']\n",
      "    ['X' '1' 'X']\n",
      "    ['X' 'X' '6']\n",
      "Agent Action : Position = 5, number = 2\n",
      "After agent action, state\n",
      "    ['7' 'X' 'X']\n",
      "    ['X' '1' '2']\n",
      "    ['X' 'X' '6']\n",
      "Give user action (position,number):(2,5)\n",
      "After user input : Position = 2, number = 5\n",
      "    ['7' 'X' '5']\n",
      "    ['X' '1' '2']\n",
      "    ['X' 'X' '6']\n",
      "Agent Action : Position = 1, number = 8\n",
      "After agent action, state\n",
      "    ['7' '8' '5']\n",
      "    ['X' '1' '2']\n",
      "    ['X' 'X' '6']\n",
      "Give user action (position,number):(6,9)\n",
      "After user input : Position = 6, number = 9\n",
      "    ['7' '8' '5']\n",
      "    ['X' '1' '2']\n",
      "    ['9' 'X' '6']\n",
      "User Won\n",
      "    ['7' '8' '5']\n",
      "    ['X' '1' '2']\n",
      "    ['9' 'X' '6']\n"
     ]
    }
   ],
   "source": [
    "current_game_state = env.state.copy()\n",
    "print(env.print_board(current_game_state))\n",
    "first_user_action = ast.literal_eval(input(\"Give user action (position,number):\"))\n",
    "if first_user_action not in env.action_space(current_game_state)[1]:\n",
    "    raise Exception(\"Invalid action : %s from the user\" %(str(first_user_action)))\n",
    "next_state = env.state_transition(current_game_state,first_user_action)\n",
    "print(\"After user input : Position = %d, number = %d\" %(first_user_action[0],first_user_action[1]))\n",
    "print(env.print_board(next_state))\n",
    "\n",
    "current_game_state = next_state\n",
    "\n",
    "game_ended = False\n",
    "Q_dict_copy = Q_dict.copy()\n",
    "while not game_ended:\n",
    "    agent_action = get_agent_action(current_game_state,Q_dict_copy)\n",
    "    next_state, reward, game_ended, msg = env.game_step(current_game_state,agent_action)\n",
    "    current_game_state = next_state\n",
    "print(msg)\n",
    "print(env.print_board(current_game_state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play Game and Also Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_object(obj,filename):\n",
    "    with open(filename,\"wb\") as f:\n",
    "        pickle.dump(obj,f,pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.09\n",
    "GAMMA = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent Action : Position = 3, number = 3\n",
      "After agent action, state\n",
      "    ['X' 'X' 'X']\n",
      "    ['3' 'X' 'X']\n",
      "    ['X' 'X' 'X']\n",
      "Give user action (position,number):(4,8)\n",
      "After user input : Position = 4, number = 8\n",
      "    ['X' 'X' 'X']\n",
      "    ['3' '8' 'X']\n",
      "    ['X' 'X' 'X']\n",
      "For Agent (Before) : when in state = x-x-x-x-x-x-x-x-x, with action = (3, 3), Q_value = -1.374272\n",
      "For Agent (After) : when in state = x-x-x-x-x-x-x-x-x, with action = (3, 3), Q_value = -1.374086\n",
      "Agent Action : Position = 5, number = 5\n",
      "After agent action, state\n",
      "    ['X' 'X' 'X']\n",
      "    ['3' '8' '5']\n",
      "    ['X' 'X' 'X']\n",
      "Give user action (position,number):(8,4)\n",
      "After user input : Position = 8, number = 4\n",
      "    ['X' 'X' 'X']\n",
      "    ['3' '8' '5']\n",
      "    ['X' 'X' '4']\n",
      "For Agent (Before) : when in state = x-x-x-3-8-x-x-x-x, with action = (5, 5), Q_value = -0.375968\n",
      "For Agent (After) : when in state = x-x-x-3-8-x-x-x-x, with action = (5, 5), Q_value = -0.432131\n",
      "Agent Action : Position = 7, number = 9\n",
      "After agent action, state\n",
      "    ['X' 'X' 'X']\n",
      "    ['3' '8' '5']\n",
      "    ['X' '9' '4']\n",
      "Give user action (position,number):(2,6)\n",
      "After user input : Position = 2, number = 6\n",
      "    ['X' 'X' '6']\n",
      "    ['3' '8' '5']\n",
      "    ['X' '9' '4']\n",
      "State = x-x-6-3-8-5-x-9-4 was seen by agent for the first time :-0, adding it for learning\n",
      "For Agent (Before) : when in state = x-x-x-3-8-5-x-x-4, with action = (7, 9), Q_value = 0.000000\n",
      "For Agent (After) : when in state = x-x-x-3-8-5-x-x-4, with action = (7, 9), Q_value = -0.900000\n",
      "User Won\n",
      "    ['X' 'X' '6']\n",
      "    ['3' '8' '5']\n",
      "    ['X' '9' '4']\n"
     ]
    }
   ],
   "source": [
    "current_game_state = env.state.copy()\n",
    "game_ended = False\n",
    "Q_dict_copy = Q_dict.copy()\n",
    "while not game_ended:\n",
    "    agent_action = get_agent_action(current_game_state,Q_dict_copy)\n",
    "    next_state, reward, game_ended, msg = env.game_step(current_game_state,agent_action)\n",
    "    \n",
    "    Q_dict_copy = add_to_dict(next_state,Q_dict_copy.copy())\n",
    "    \n",
    "    if game_ended:\n",
    "            best_next_action = None\n",
    "            # Remember in this case, the Q_value for next state will be 0, hence we do (reward + GAMMA* 0)\n",
    "            print(\"For Agent (Before) : when in state = %s, with action = %s, Q_value = %f\" %(Q_state(current_game_state),str(agent_action),Q_dict_copy[Q_state(current_game_state)][agent_action]))\n",
    "            Q_dict_copy[Q_state(current_game_state)][agent_action] += LR * ((reward + (GAMMA*0)) - Q_dict_copy[Q_state(current_game_state)][agent_action]) \n",
    "            print(\"For Agent (After) : when in state = %s, with action = %s, Q_value = %f\" %(Q_state(current_game_state),str(agent_action),Q_dict_copy[Q_state(current_game_state)][agent_action]))\n",
    "    else:\n",
    "        # we update the current_state,agent_action reward based on \"BEST\" action possible for the next state #\n",
    "        # we get the BEST ACTION for next_state and use that with the discount factor #\n",
    "        best_next_action = max(Q_dict_copy[Q_state(next_state)],key=Q_dict_copy[Q_state(next_state)].get)\n",
    "        print(\"For Agent (Before) : when in state = %s, with action = %s, Q_value = %f\" %(Q_state(current_game_state),str(agent_action),Q_dict_copy[Q_state(current_game_state)][agent_action]))\n",
    "        Q_dict_copy[Q_state(current_game_state)][agent_action] += LR * ((reward + (GAMMA*(Q_dict_copy[Q_state(next_state)][best_next_action]))) - Q_dict_copy[Q_state(current_game_state)][agent_action] ) \n",
    "        print(\"For Agent (After) : when in state = %s, with action = %s, Q_value = %f\" %(Q_state(current_game_state),str(agent_action),Q_dict_copy[Q_state(current_game_state)][agent_action]))\n",
    "    current_game_state = next_state\n",
    "print(msg)\n",
    "print(env.print_board(current_game_state))\n",
    "Q_dict = Q_dict_copy.copy()\n",
    "#filename = \"Q_Policy_\" + datetime.now().strftime(\"%Y%m%d_%H%M%S\") + \".pickle.pk\"\n",
    "#save_object(Q_dict,filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_object(Q_dict,\"Q_policy_TicTactoe_201902015_143001.pickle,pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 1): -1.389461173046447,\n",
       " (0, 3): -1.3848897055748177,\n",
       " (0, 5): -1.1978579030536172,\n",
       " (0, 7): -1.3823387339632986,\n",
       " (0, 9): -0.08788708562547459,\n",
       " (1, 1): -1.3301509818778756,\n",
       " (1, 3): -1.3785077007916595,\n",
       " (1, 5): -0.09170300651423155,\n",
       " (1, 7): -1.3826571147113114,\n",
       " (1, 9): -1.3249402828129628,\n",
       " (2, 1): -1.3799988003737713,\n",
       " (2, 3): -1.3331438174255488,\n",
       " (2, 5): -0.8827370140452909,\n",
       " (2, 7): -1.3533192778143368,\n",
       " (2, 9): -1.0147217639815886,\n",
       " (3, 1): -1.3312328667194415,\n",
       " (3, 3): -1.3742721122115256,\n",
       " (3, 5): -1.2253514205959806,\n",
       " (3, 7): -1.368011451340596,\n",
       " (3, 9): -1.3139866134873774,\n",
       " (4, 1): -1.3855284147408626,\n",
       " (4, 3): -1.3833060201196665,\n",
       " (4, 5): -1.3386942315203538,\n",
       " (4, 7): -1.3859401724507294,\n",
       " (4, 9): -1.1140268501381447,\n",
       " (5, 1): -1.3744107070159663,\n",
       " (5, 3): -1.3492899060777384,\n",
       " (5, 5): -1.3900035053030109,\n",
       " (5, 7): -1.3908355801047179,\n",
       " (5, 9): -1.3069316909883097,\n",
       " (6, 1): -1.3612513129501687,\n",
       " (6, 3): -1.3955129594397437,\n",
       " (6, 5): -1.3081785599808584,\n",
       " (6, 7): -1.3855979702811052,\n",
       " (6, 9): -1.2883909085190821,\n",
       " (7, 1): -1.3676682806245648,\n",
       " (7, 3): -1.364285974136507,\n",
       " (7, 5): -1.382680867237676,\n",
       " (7, 7): -1.3803645942841951,\n",
       " (7, 9): -1.3701615400800393,\n",
       " (8, 1): -1.387939685168501,\n",
       " (8, 3): -1.388307195085564,\n",
       " (8, 5): -1.352480778938464,\n",
       " (8, 7): -1.365458451703692,\n",
       " (8, 9): -1.3750767862587343}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_dict[\"x-x-x-x-x-x-x-x-x\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(1, 1): -0.047736420512356326,\n",
       " (1, 3): -2.791825571023639,\n",
       " (1, 5): -1.5225140215398645,\n",
       " (1, 7): -1.251409390109311,\n",
       " (2, 1): -0.0714491543887863,\n",
       " (2, 3): -1.9928409649568601,\n",
       " (2, 5): -0.4724634542874121,\n",
       " (2, 7): -0.8886108018724314,\n",
       " (3, 1): -1.3615342281965954,\n",
       " (3, 3): -1.9461107673442282,\n",
       " (3, 5): -2.4972480588000643,\n",
       " (3, 7): -0.3976677329550965,\n",
       " (5, 1): -1.2252162936838946,\n",
       " (5, 3): -1.9854617263917105,\n",
       " (5, 5): -2.41059689500214,\n",
       " (5, 7): -1.4829769377497106,\n",
       " (6, 1): -0.5599883934190423,\n",
       " (6, 3): -1.4819395197183716,\n",
       " (6, 5): 0.09616074752446535,\n",
       " (6, 7): -1.04886615354119,\n",
       " (7, 1): -1.0658677498176283,\n",
       " (7, 3): -1.1490417497891972,\n",
       " (7, 5): -1.09300684283041,\n",
       " (7, 7): -1.0672945865830934,\n",
       " (8, 1): -0.5331701875857696,\n",
       " (8, 3): -0.9112574787707827,\n",
       " (8, 5): 0.8392769549866919,\n",
       " (8, 7): -0.9339828818482201}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_dict[\"9-x-x-x-4-x-x-x-x\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
