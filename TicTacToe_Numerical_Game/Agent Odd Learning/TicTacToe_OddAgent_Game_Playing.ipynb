{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T06:38:46.004722Z",
     "start_time": "2019-02-26T06:38:45.189914Z"
    }
   },
   "outputs": [],
   "source": [
    "from TCGame_Env_GAME import TicTacToe\n",
    "import collections\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import collections\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T06:38:46.012869Z",
     "start_time": "2019-02-26T06:38:46.004722Z"
    }
   },
   "outputs": [],
   "source": [
    "env = TicTacToe()\n",
    "current_game_state = env.state.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T06:39:28.115097Z",
     "start_time": "2019-02-26T06:38:46.015869Z"
    }
   },
   "outputs": [],
   "source": [
    "# load model - Q_dict #\n",
    "with open(\"../Agent To Agent Competition/odd_Agent_20190226_145335/odd_Q_Dict_Model_20190226_150428.pickle\",\"rb\") as f:\n",
    "    Q_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T06:39:30.032048Z",
     "start_time": "2019-02-26T06:39:30.026042Z"
    }
   },
   "outputs": [],
   "source": [
    "def Q_state(state):\n",
    "    return ('-'.join(str(e) for e in state)).replace('nan','x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T06:39:31.372603Z",
     "start_time": "2019-02-26T06:39:30.287871Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_agent_action(game_state,q_dict_ref,random_select=False):\n",
    "    if random_select:\n",
    "        proposed_action = random.choice([ac for itr,ac in enumerate(env.action_space(game_state)[0])])\n",
    "    else:\n",
    "        proposed_action = max(q_dict_ref[Q_state(game_state)],key=q_dict_ref[Q_state(game_state)].get)\n",
    "    return proposed_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T06:39:32.089303Z",
     "start_time": "2019-02-26T06:39:31.378609Z"
    }
   },
   "outputs": [],
   "source": [
    "def execute_action(game_state,action,from_user=True):\n",
    "    # action = (position,number)\n",
    "    agent_allowed, user_allowed = env.action_space(game_state)\n",
    "    if from_user and action in user_allowed:\n",
    "        return env.state_transition(game_state,action)\n",
    "    elif not from_user and action in agent_allowed:\n",
    "        return env.state_transition(game_state,action)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid input : %s , from_user : %s\" %(str(action),str(from_user)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T06:39:33.255626Z",
     "start_time": "2019-02-26T06:39:32.089303Z"
    }
   },
   "outputs": [],
   "source": [
    "def valid_actions(state):\n",
    "\n",
    "    valid_Actions = []\n",
    "    \n",
    "    valid_Actions = [i for i in env.action_space(state)[0]]\n",
    "    return valid_Actions\n",
    "\n",
    "def add_to_dict(state,q_dict_ref):\n",
    "    state1 = Q_state(state)\n",
    "    \n",
    "    valid_act = valid_actions(state)\n",
    "    if state1 not in q_dict_ref.keys():\n",
    "        print(\"State = %s was seen by agent for the first time :-0, adding it for learning\" %(state1))\n",
    "        for action in valid_act:\n",
    "            q_dict_ref[state1][action]=0\n",
    "    \n",
    "    return q_dict_ref"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Testing) ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T05:43:40.515857Z",
     "start_time": "2019-02-26T05:43:40.501868Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 1): -1.7793120816908399,\n",
       " (0, 3): -1.7795132605128308,\n",
       " (0, 5): -1.7797471147748856,\n",
       " (0, 7): -1.7797636472619942,\n",
       " (0, 9): -1.779371353423175,\n",
       " (1, 1): -1.7791723060375089,\n",
       " (1, 3): -1.779101777870136,\n",
       " (1, 5): -1.7792403689720222,\n",
       " (1, 7): -1.7792784235627197,\n",
       " (1, 9): -1.7792276197599057,\n",
       " (2, 1): -1.779310406650113,\n",
       " (2, 3): -1.7794176904429535,\n",
       " (2, 5): -1.7797983543831868,\n",
       " (2, 7): -1.7792356197351098,\n",
       " (2, 9): -1.7790743463153418,\n",
       " (3, 1): -1.7790889041923859,\n",
       " (3, 3): -1.779370679172739,\n",
       " (3, 5): -1.7794700960320782,\n",
       " (3, 7): -1.7790751953529171,\n",
       " (3, 9): -1.779117774752209,\n",
       " (4, 1): -1.7792588504670928,\n",
       " (4, 3): -1.7794829756487602,\n",
       " (4, 5): -1.7793453325458113,\n",
       " (4, 7): -1.7795189479536877,\n",
       " (4, 9): -1.779140963870029,\n",
       " (5, 1): -1.7795534564518114,\n",
       " (5, 3): -1.779249782461423,\n",
       " (5, 5): -1.7792227689559212,\n",
       " (5, 7): -1.7792124044439985,\n",
       " (5, 9): -1.7794397593561853,\n",
       " (6, 1): -1.779479997158969,\n",
       " (6, 3): -1.779472456318031,\n",
       " (6, 5): -1.779064462923111,\n",
       " (6, 7): -1.7791752775253664,\n",
       " (6, 9): -1.7800033738012453,\n",
       " (7, 1): -1.7791342183794503,\n",
       " (7, 3): -1.7797296787343326,\n",
       " (7, 5): -1.7796361974235773,\n",
       " (7, 7): -1.779496845223967,\n",
       " (7, 9): -1.7794830284919356,\n",
       " (8, 1): -1.779300368799477,\n",
       " (8, 3): -1.7794122356519084,\n",
       " (8, 5): -1.7795284998261751,\n",
       " (8, 7): -1.779381316899622,\n",
       " (8, 9): -1.7791527176640676}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_dict[Q_state(env.state)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T05:44:12.483611Z",
     "start_time": "2019-02-26T05:43:52.518639Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent Action : Position = 6, number = 5\n",
      "After agent action, state\n",
      "    ['X' 'X' 'X']\n",
      "    ['X' 'X' 'X']\n",
      "    ['5' 'X' 'X']\n",
      "Give user action (position,number):(4,6)\n",
      "After user input : Position = 4, number = 6\n",
      "    ['X' 'X' 'X']\n",
      "    ['X' '6' 'X']\n",
      "    ['5' 'X' 'X']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([nan, nan, nan, nan, 6, nan, 5, nan, nan], -1, False, 'Continue')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(env.state)\n",
    "env.game_step(env.state,get_agent_action(env.state,Q_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just Play the game #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T06:43:34.204078Z",
     "start_time": "2019-02-26T06:43:21.951873Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent Action : Position = 0, number = 9\n",
      "After agent action, state\n",
      "    ['9' 'X' 'X']\n",
      "    ['X' 'X' 'X']\n",
      "    ['X' 'X' 'X']\n",
      "Give user action (position,number):(4,2)\n",
      "After user input : Position = 4, number = 2\n",
      "    ['9' 'X' 'X']\n",
      "    ['X' '2' 'X']\n",
      "    ['X' 'X' 'X']\n",
      "Agent Action : Position = 8, number = 3\n",
      "After agent action, state\n",
      "    ['9' 'X' 'X']\n",
      "    ['X' '2' 'X']\n",
      "    ['X' 'X' '3']\n",
      "Give user action (position,number):(7,8)\n",
      "After user input : Position = 7, number = 8\n",
      "    ['9' 'X' 'X']\n",
      "    ['X' '2' 'X']\n",
      "    ['X' '8' '3']\n",
      "Agent Action : Position = 1, number = 5\n",
      "After agent action, state\n",
      "    ['9' '5' 'X']\n",
      "    ['X' '2' 'X']\n",
      "    ['X' '8' '3']\n",
      "Agent Won\n",
      "    ['9' '5' 'X']\n",
      "    ['X' '2' 'X']\n",
      "    ['X' '8' '3']\n"
     ]
    }
   ],
   "source": [
    "current_game_state = env.state.copy()\n",
    "game_ended = False\n",
    "Q_dict_copy = Q_dict.copy()\n",
    "while not game_ended:\n",
    "    agent_action = get_agent_action(current_game_state,Q_dict_copy)\n",
    "    next_state, reward, game_ended, msg = env.game_step(current_game_state,agent_action)\n",
    "    current_game_state = next_state\n",
    "print(msg)\n",
    "print(env.print_board(current_game_state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play Game and Also Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-15T04:26:32.946409Z",
     "start_time": "2019-02-15T04:26:32.941402Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_object(obj,filename):\n",
    "    with open(filename,\"wb\") as f:\n",
    "        pickle.dump(obj,f,pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.09\n",
    "GAMMA = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-15T05:31:06.902470Z",
     "start_time": "2019-02-15T05:29:50.716044Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent Action : Position = 3, number = 3\n",
      "After agent action, state\n",
      "    ['X' 'X' 'X']\n",
      "    ['3' 'X' 'X']\n",
      "    ['X' 'X' 'X']\n",
      "Give user action (position,number):(4,8)\n",
      "After user input : Position = 4, number = 8\n",
      "    ['X' 'X' 'X']\n",
      "    ['3' '8' 'X']\n",
      "    ['X' 'X' 'X']\n",
      "For Agent (Before) : when in state = x-x-x-x-x-x-x-x-x, with action = (3, 3), Q_value = -1.374272\n",
      "For Agent (After) : when in state = x-x-x-x-x-x-x-x-x, with action = (3, 3), Q_value = -1.374086\n",
      "Agent Action : Position = 5, number = 5\n",
      "After agent action, state\n",
      "    ['X' 'X' 'X']\n",
      "    ['3' '8' '5']\n",
      "    ['X' 'X' 'X']\n",
      "Give user action (position,number):(8,4)\n",
      "After user input : Position = 8, number = 4\n",
      "    ['X' 'X' 'X']\n",
      "    ['3' '8' '5']\n",
      "    ['X' 'X' '4']\n",
      "For Agent (Before) : when in state = x-x-x-3-8-x-x-x-x, with action = (5, 5), Q_value = -0.375968\n",
      "For Agent (After) : when in state = x-x-x-3-8-x-x-x-x, with action = (5, 5), Q_value = -0.432131\n",
      "Agent Action : Position = 7, number = 9\n",
      "After agent action, state\n",
      "    ['X' 'X' 'X']\n",
      "    ['3' '8' '5']\n",
      "    ['X' '9' '4']\n",
      "Give user action (position,number):(2,6)\n",
      "After user input : Position = 2, number = 6\n",
      "    ['X' 'X' '6']\n",
      "    ['3' '8' '5']\n",
      "    ['X' '9' '4']\n",
      "State = x-x-6-3-8-5-x-9-4 was seen by agent for the first time :-0, adding it for learning\n",
      "For Agent (Before) : when in state = x-x-x-3-8-5-x-x-4, with action = (7, 9), Q_value = 0.000000\n",
      "For Agent (After) : when in state = x-x-x-3-8-5-x-x-4, with action = (7, 9), Q_value = -0.900000\n",
      "User Won\n",
      "    ['X' 'X' '6']\n",
      "    ['3' '8' '5']\n",
      "    ['X' '9' '4']\n"
     ]
    }
   ],
   "source": [
    "current_game_state = env.state.copy()\n",
    "game_ended = False\n",
    "Q_dict_copy = Q_dict.copy()\n",
    "while not game_ended:\n",
    "    agent_action = get_agent_action(current_game_state,Q_dict_copy)\n",
    "    next_state, reward, game_ended, msg = env.game_step(current_game_state,agent_action)\n",
    "    \n",
    "    Q_dict_copy = add_to_dict(next_state,Q_dict_copy.copy())\n",
    "    \n",
    "    if game_ended:\n",
    "            best_next_action = None\n",
    "            # Remember in this case, the Q_value for next state will be 0, hence we do (reward + GAMMA* 0)\n",
    "            print(\"For Agent (Before) : when in state = %s, with action = %s, Q_value = %f\" %(Q_state(current_game_state),str(agent_action),Q_dict_copy[Q_state(current_game_state)][agent_action]))\n",
    "            Q_dict_copy[Q_state(current_game_state)][agent_action] += LR * ((reward + (GAMMA*0)) - Q_dict_copy[Q_state(current_game_state)][agent_action]) \n",
    "            print(\"For Agent (After) : when in state = %s, with action = %s, Q_value = %f\" %(Q_state(current_game_state),str(agent_action),Q_dict_copy[Q_state(current_game_state)][agent_action]))\n",
    "    else:\n",
    "        # we update the current_state,agent_action reward based on \"BEST\" action possible for the next state #\n",
    "        # we get the BEST ACTION for next_state and use that with the discount factor #\n",
    "        best_next_action = max(Q_dict_copy[Q_state(next_state)],key=Q_dict_copy[Q_state(next_state)].get)\n",
    "        print(\"For Agent (Before) : when in state = %s, with action = %s, Q_value = %f\" %(Q_state(current_game_state),str(agent_action),Q_dict_copy[Q_state(current_game_state)][agent_action]))\n",
    "        Q_dict_copy[Q_state(current_game_state)][agent_action] += LR * ((reward + (GAMMA*(Q_dict_copy[Q_state(next_state)][best_next_action]))) - Q_dict_copy[Q_state(current_game_state)][agent_action] ) \n",
    "        print(\"For Agent (After) : when in state = %s, with action = %s, Q_value = %f\" %(Q_state(current_game_state),str(agent_action),Q_dict_copy[Q_state(current_game_state)][agent_action]))\n",
    "    current_game_state = next_state\n",
    "print(msg)\n",
    "print(env.print_board(current_game_state))\n",
    "Q_dict = Q_dict_copy.copy()\n",
    "#filename = \"Q_Policy_\" + datetime.now().strftime(\"%Y%m%d_%H%M%S\") + \".pickle.pk\"\n",
    "#save_object(Q_dict,filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-15T05:31:44.519437Z",
     "start_time": "2019-02-15T05:31:40.636462Z"
    }
   },
   "outputs": [],
   "source": [
    "save_object(Q_dict,\"Q_policy_TicTactoe_201902015_143001.pickle,pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-15T05:02:37.463229Z",
     "start_time": "2019-02-15T05:02:37.453243Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 1): -1.389461173046447,\n",
       " (0, 3): -1.3848897055748177,\n",
       " (0, 5): -1.1978579030536172,\n",
       " (0, 7): -1.3823387339632986,\n",
       " (0, 9): -0.08788708562547459,\n",
       " (1, 1): -1.3301509818778756,\n",
       " (1, 3): -1.3785077007916595,\n",
       " (1, 5): -0.09170300651423155,\n",
       " (1, 7): -1.3826571147113114,\n",
       " (1, 9): -1.3249402828129628,\n",
       " (2, 1): -1.3799988003737713,\n",
       " (2, 3): -1.3331438174255488,\n",
       " (2, 5): -0.8827370140452909,\n",
       " (2, 7): -1.3533192778143368,\n",
       " (2, 9): -1.0147217639815886,\n",
       " (3, 1): -1.3312328667194415,\n",
       " (3, 3): -1.3742721122115256,\n",
       " (3, 5): -1.2253514205959806,\n",
       " (3, 7): -1.368011451340596,\n",
       " (3, 9): -1.3139866134873774,\n",
       " (4, 1): -1.3855284147408626,\n",
       " (4, 3): -1.3833060201196665,\n",
       " (4, 5): -1.3386942315203538,\n",
       " (4, 7): -1.3859401724507294,\n",
       " (4, 9): -1.1140268501381447,\n",
       " (5, 1): -1.3744107070159663,\n",
       " (5, 3): -1.3492899060777384,\n",
       " (5, 5): -1.3900035053030109,\n",
       " (5, 7): -1.3908355801047179,\n",
       " (5, 9): -1.3069316909883097,\n",
       " (6, 1): -1.3612513129501687,\n",
       " (6, 3): -1.3955129594397437,\n",
       " (6, 5): -1.3081785599808584,\n",
       " (6, 7): -1.3855979702811052,\n",
       " (6, 9): -1.2883909085190821,\n",
       " (7, 1): -1.3676682806245648,\n",
       " (7, 3): -1.364285974136507,\n",
       " (7, 5): -1.382680867237676,\n",
       " (7, 7): -1.3803645942841951,\n",
       " (7, 9): -1.3701615400800393,\n",
       " (8, 1): -1.387939685168501,\n",
       " (8, 3): -1.388307195085564,\n",
       " (8, 5): -1.352480778938464,\n",
       " (8, 7): -1.365458451703692,\n",
       " (8, 9): -1.3750767862587343}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_dict[\"x-x-x-x-x-x-x-x-x\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(1, 1): -0.047736420512356326,\n",
       " (1, 3): -2.791825571023639,\n",
       " (1, 5): -1.5225140215398645,\n",
       " (1, 7): -1.251409390109311,\n",
       " (2, 1): -0.0714491543887863,\n",
       " (2, 3): -1.9928409649568601,\n",
       " (2, 5): -0.4724634542874121,\n",
       " (2, 7): -0.8886108018724314,\n",
       " (3, 1): -1.3615342281965954,\n",
       " (3, 3): -1.9461107673442282,\n",
       " (3, 5): -2.4972480588000643,\n",
       " (3, 7): -0.3976677329550965,\n",
       " (5, 1): -1.2252162936838946,\n",
       " (5, 3): -1.9854617263917105,\n",
       " (5, 5): -2.41059689500214,\n",
       " (5, 7): -1.4829769377497106,\n",
       " (6, 1): -0.5599883934190423,\n",
       " (6, 3): -1.4819395197183716,\n",
       " (6, 5): 0.09616074752446535,\n",
       " (6, 7): -1.04886615354119,\n",
       " (7, 1): -1.0658677498176283,\n",
       " (7, 3): -1.1490417497891972,\n",
       " (7, 5): -1.09300684283041,\n",
       " (7, 7): -1.0672945865830934,\n",
       " (8, 1): -0.5331701875857696,\n",
       " (8, 3): -0.9112574787707827,\n",
       " (8, 5): 0.8392769549866919,\n",
       " (8, 7): -0.9339828818482201}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_dict[\"9-x-x-x-4-x-x-x-x\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
