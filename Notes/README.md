### Concepts in Reinforced Learning ###

#### Introduction (Series - 1) ####
- What is Reinforced Learning from Agent - Environment - Action - Reward relationships. The mathematical formulation of State Value, Action Value functions and their relationships.
- Deterministic Vs Stochastic Environments
- Description of model-based methods and model free methods.
- Understanding of Control and Prediction problem in RL setup

Note: The above should be around 1 ~ 1.5 hours of discussion with presentation covering mathematical formulation with some examples

#### Model Based Methods (Series - 2) ####
- Policy Iterations Methods (Nested implementation prediction and control structures) - along with a case study and mathematical formulation.
- Value Iterations methods (Implicit control structure implementation within prediction structure) - along with case study and mathematical formulation
- Mixed approach (Policy Iteration + Value Iteration) - Theoretical Concept.

#### Model Free Methods - (Deterministic State) (Series - 3) ####
- Concept of exploration and exploitation implementation in RL problems.
- Monte Carlo Learning Methodology - Mathematical formulation, along with small cast study.
- Temporal Learning - Theoretical Introduction
 - Q-Learning - Greedy approach for updation of Q-Value function (Action value  function) - along with small case study covering concept of exploitation and exploration (Epsilon Strategy) & State Action dict based model methods.

#### Model Free Methods - (Non Deterministic State) (Series - 4) ####
- Deep Q-Learning - Ideology behind deep q-learning and using of neural networks for RL problem prediction. 
 - Model architectures 
  - Model 1 - State + Action as Input with Q-Value prediction as output
  - Model 2 - State as Input with all possible action's Q-Value as output.
 - Deep Q-Learning model architectures - Single model and double model network architectures.

#### Other modelling techniques (Series - 5) ####
- Policy Gradient methods, Actor Critic Methods and SARSA.